{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Suite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set up the environment\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.mllib.clustering import LDA, LDAModel\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import io\n",
    "\n",
    "# Input the data\n",
    "data = sc.textFile(\"sample_data.txt\")\n",
    "parsedData = data.map(lambda line: Vectors.dense([float(x) for x in line.strip().split(' ')]))\n",
    "corpus = parsedData.zipWithIndex().map(lambda x: [x[1], x[0]]).cache()\n",
    "\n",
    "# Cluster the documents into three topics using LDA\n",
    "ldaModel = LDA.train(corpus, k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned topics (as distributions over vocab of 11 words):\n",
      "Topic 0:\n",
      " 5.62631004755\n",
      " 18.8616474703\n",
      " 4.28187623872\n",
      " 5.01676737764\n",
      " 5.28503421228\n",
      " 4.01115444738\n",
      " 9.33112263062\n",
      " 1.39866720377\n",
      " 4.54494186264\n",
      " 17.1316831112\n",
      " 11.2615159387\n",
      "Topic 1:\n",
      " 12.0176371072\n",
      " 6.27438369428\n",
      " 4.37603086404\n",
      " 17.7011979581\n",
      " 14.9105894751\n",
      " 13.2031216267\n",
      " 7.44542907552\n",
      " 5.47148322893\n",
      " 0.769780523621\n",
      " 3.13133106182\n",
      " 1.78253720154\n",
      "Topic 2:\n",
      " 8.35605284529\n",
      " 3.86396883542\n",
      " 3.34209289724\n",
      " 17.2820346642\n",
      " 4.80437631266\n",
      " 4.78572392591\n",
      " 14.2234482939\n",
      " 3.1298495673\n",
      " 2.68527761374\n",
      " 3.73698582701\n",
      " 19.9559468597\n"
     ]
    }
   ],
   "source": [
    "# Output topics. Each is a distribution over words (matching word count vectors)\n",
    "print(\"Learned topics (as distributions over vocab of \" + str(ldaModel.vocabSize()) + \" words):\")\n",
    "topics = ldaModel.topicsMatrix()\n",
    "for topic in range(3):\n",
    "    print(\"Topic \" + str(topic) + \":\")\n",
    "    for word in range(0, ldaModel.vocabSize()):\n",
    "        print(\" \" + str(topics[word][topic]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 6812\n"
     ]
    }
   ],
   "source": [
    "# Sample docs\n",
    "documents = sp.io.mmread(\"sample_docs_sparse.obj\")\n",
    "N, V = documents.shape\n",
    "print N, V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STM Class Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sepal.Length</th>\n",
       "      <th>Sepal.Width</th>\n",
       "      <th>Petal.Length</th>\n",
       "      <th>Petal.Width</th>\n",
       "      <th>Species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sepal.Length  Sepal.Width  Petal.Length  Petal.Width Species\n",
       "1           5.1          3.5           1.4          0.2  setosa\n",
       "2           4.9          3.0           1.4          0.2  setosa\n",
       "3           4.7          3.2           1.3          0.2  setosa\n",
       "4           4.6          3.1           1.5          0.2  setosa\n",
       "5           5.0          3.6           1.4          0.2  setosa"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas.rpy.common as rcom\n",
    "iris = rcom.load_data('iris')\n",
    "iris.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class STM(object):\n",
    "    \n",
    "    def __init__(self, sc):\n",
    "        self.sc = sc\n",
    "        self.status = 0\n",
    "        self.theta = None\n",
    "        self.n_partitions = None # Get this from Spark context\n",
    "        self.lhood_bound = None\n",
    "        self.seed = None\n",
    "        self.description = None\n",
    "    \n",
    "    def __e_step__(self):\n",
    "        self.stopits = True\n",
    "        \n",
    "    def __m_step__(self):\n",
    "        self.stopits = True\n",
    "        \n",
    "    def __optimize_stm__(self, optimizer):\n",
    "        # Run the EM algorithm\n",
    "        if optimizer == 'em':\n",
    "            print \"Beginning EM\"\n",
    "            while self.stopits == False:\n",
    "                self.__e_step__()\n",
    "                self.__m_step__()\n",
    "    \n",
    "    def __theta_posterior_draw__(self):\n",
    "        # TODO\n",
    "        return None\n",
    "    \n",
    "    def __compute_lhood_bound__(self):\n",
    "        # TODO\n",
    "        return None\n",
    "    \n",
    "    def __init_random__(self):\n",
    "        self.mu = np.zeros((self.K-1, 1))\n",
    "        self.sigma = 20 * np.identity(self.K-1)\n",
    "        self.beta = np.random.gamma(shape=0.1, size=(self.A, self.K, self.V))\n",
    "        self.Lambda = np.zeros((self.N, self.K-1))\n",
    "        \n",
    "    def __jeffreys_prior__(self):\n",
    "        return None\n",
    "        \n",
    "    def __kappa_init__(self):\n",
    "        self.kappa = {}\n",
    "        \n",
    "        # Baseline log probabilities\n",
    "        m = np.array(self.wcounts)[0].astype(float)/ \n",
    "                                 np.sum(np.array(self.wcounts)[0])\n",
    "        m = np.log(m) - np.log(m)\n",
    "        self.kappa['m'] = m; del m;\n",
    "        \n",
    "        # Parameter objects\n",
    "        self.aspectmod = self.A > 1\n",
    "        \n",
    "        # Covariates\n",
    "        kappa['covar'] = None\n",
    "        \n",
    "        \n",
    "    def print_topics(self):\n",
    "        # TODO\n",
    "        return None\n",
    "    \n",
    "    def estimate_effect(self):\n",
    "        # TODO\n",
    "        self.N = 30\n",
    "        return None\n",
    "    \n",
    "    def train(self, documents, vocab=None, K=10, prevalence=None, content=None, data=None,\n",
    "              max_em_its=500, init_type=\"random\", optimizer=\"em\", \n",
    "              em_tol=1e-5, verbose=True, report_every=5, LDAbeta=True,\n",
    "              interactions=True, ngroups=1, model=None):\n",
    "        \"\"\"Train a STM model.\n",
    "        \"\"\"\n",
    "        # Basic setup\n",
    "        self.N, self.V = documents.shape\n",
    "        self.K = K\n",
    "        self.verbose = verbose\n",
    "        self.report_every = report_every\n",
    "        self.LDAbeta = LDAbeta\n",
    "        self.wcounts = documents.sum(0)\n",
    "        self.ntokens = np.sum(self.wcounts)\n",
    "        self.stopits = False\n",
    "        self.A = len(content)\n",
    "        self.P = len(prevalence)\n",
    "        self.X = data[prevalence]\n",
    "        self.Y = data[content]\n",
    "        \n",
    "        # Initialization\n",
    "        print \"Beginning Initialization\"\n",
    "        if init_type == 'random':\n",
    "            self.__init_random__()\n",
    "        self.__kappa_init__()\n",
    "        \n",
    "        # Run EM\n",
    "        self.__optimize_stm__(optimizer)\n",
    "        \n",
    "        # Declare completion\n",
    "        print \"All done!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning Initialization\n",
      "Beginning EM\n",
      "All done!\n"
     ]
    }
   ],
   "source": [
    "stm = STM(sc)\n",
    "stm.train(documents, prevalence=['Species'], content=['Species'], data=iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ nan,  nan,  nan, ...,  nan,  nan,   1.]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stm.wcounts.astype(float) / sum(stm.wcounts.astype(float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = np.log(np.array(stm.wcounts)[0].astype(float)/ \n",
    "                                 np.sum(np.array(stm.wcounts)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-inf"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pySpark (Spark 1.5.0)",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

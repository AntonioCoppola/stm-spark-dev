{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Suite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set up the environment\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.mllib.clustering import LDA, LDAModel\n",
    "\n",
    "# Input the data\n",
    "data = sc.textFile(\"sample_data.txt\")\n",
    "parsedData = data.map(lambda line: Vectors.dense([float(x) for x in line.strip().split(' ')]))\n",
    "corpus = parsedData.zipWithIndex().map(lambda x: [x[1], x[0]]).cache()\n",
    "\n",
    "# Cluster the documents into three topics using LDA\n",
    "ldaModel = LDA.train(corpus, k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned topics (as distributions over vocab of 11 words):\n",
      "Topic 0:\n",
      " 3.38698966046\n",
      " 16.5149247809\n",
      " 8.22344000604\n",
      " 13.324286369\n",
      " 3.97307554853\n",
      " 3.49907148023\n",
      " 16.3142825635\n",
      " 0.788149741699\n",
      " 3.18094267392\n",
      " 11.4404077594\n",
      " 4.97669070255\n",
      "Topic 1:\n",
      " 16.8035576418\n",
      " 8.41641273262\n",
      " 2.24631799057\n",
      " 8.94200870528\n",
      " 17.439089753\n",
      " 16.068298448\n",
      " 2.88075562955\n",
      " 8.40130585933\n",
      " 1.00959257646\n",
      " 3.69872481465\n",
      " 1.43484959473\n",
      "Topic 2:\n",
      " 5.80945269775\n",
      " 4.06866248649\n",
      " 1.5302420034\n",
      " 17.7337049257\n",
      " 3.58783469847\n",
      " 2.43263007179\n",
      " 11.8049618069\n",
      " 0.810544398976\n",
      " 3.80946474962\n",
      " 8.8608674259\n",
      " 26.5884597027\n"
     ]
    }
   ],
   "source": [
    "# Output topics. Each is a distribution over words (matching word count vectors)\n",
    "print(\"Learned topics (as distributions over vocab of \" + str(ldaModel.vocabSize()) + \" words):\")\n",
    "topics = ldaModel.topicsMatrix()\n",
    "for topic in range(3):\n",
    "    print(\"Topic \" + str(topic) + \":\")\n",
    "    for word in range(0, ldaModel.vocabSize()):\n",
    "        print(\" \" + str(topics[word][topic]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STM Class Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class STM(object):\n",
    "    \n",
    "    def __init__(self, sc):\n",
    "        self.sc = sc\n",
    "        self.status = 0\n",
    "        self.beta = None\n",
    "        self.sigma = None\n",
    "        self.mu = None\n",
    "        self.theta = None\n",
    "        self.n_partitions = None # Get this from Spark context\n",
    "        self.Lambda = None\n",
    "        self.lhood_bound = None\n",
    "        self.K = None\n",
    "        self.seed = None\n",
    "        self.description = None\n",
    "    \n",
    "    def __e_step__(self):\n",
    "        # TODO\n",
    "        return None\n",
    "        \n",
    "    def __m_step__(self):\n",
    "        # TODO\n",
    "        return None\n",
    "        \n",
    "    def __optimize_stm__(self):\n",
    "        # TODO\n",
    "        return None\n",
    "    \n",
    "    def __theta_posterior_draw__(self):\n",
    "        # TODO\n",
    "        return None\n",
    "        \n",
    "    def print_topics(self):\n",
    "        # TODO\n",
    "        return None\n",
    "    \n",
    "    def estimate_effect(self):\n",
    "        # TODO\n",
    "        return None\n",
    "    \n",
    "    def train(self, documents, vocab, K, prevalence, content, data,\n",
    "              max_em_its=500, init_type=\"Spectral\", optimizer=\"em\", \n",
    "              em_tol=1e-5, verbose=True, report_every=5, LDAbeta=True,\n",
    "              interactions=True, ngroups=1, model=None):\n",
    "        \"\"\"Train a STM model.\n",
    "        \"\"\"\n",
    "        N = len(documents)\n",
    "        return None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pySpark (Spark 1.5.0)",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The rpy2.ipython extension is already loaded. To reload it, use:\n",
      "  %reload_ext rpy2.ipython\n"
     ]
    }
   ],
   "source": [
    "# Set up the environment\n",
    "%load_ext rpy2.ipython\n",
    "import rpy2.robjects as robjects\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.mllib.clustering import LDA, LDAModel\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import time\n",
    "from scipy import optimize\n",
    "import os\n",
    "import pandas as pd\n",
    "from scipy import io\n",
    "from rpy2.robjects import pandas2ri\n",
    "import rpy2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## R Imports for STM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for f in os.listdir(\"R\"):\n",
    "    if f not in ['.DS_Store', '.Rapp.history', 'box', 'e_step_spark.R']:\n",
    "        robjects.r.source(\"R/\" + f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['splines', 'stringr', 'Matrix', 'tools', 'stats', 'graphics',\n",
       "       'grDevices', 'utils', 'datasets', 'methods', 'base'], \n",
       "      dtype='|S9')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "robjects.r('''\n",
    "    library(Matrix); library(stringr); library(splines)\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample run with R code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Subsample the corpus to only consider articles mentioning 'monte carlo'\n",
    "cond_mat_mc = pd.read_csv('cond_mat_mc.csv')\n",
    "%Rpush cond_mat_mc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building corpus... \n",
      "Converting to Lower Case... \n",
      "Removing stopwords... \n",
      "Removing numbers... \n",
      "Removing punctuation... \n",
      "Stemming... \n",
      "Creating Output... \n",
      "Removing 8267 of 15095 terms (8267 of 363331 tokens) due to frequency \n",
      "Your corpus now has 6359 documents, 6828 terms and 355064 tokens."
     ]
    }
   ],
   "source": [
    "# Prep the corpus\n",
    "robjects.r('''\n",
    "    processed_corpus_temp = textProcessor(cond_mat_mc$abstract, metadata=cond_mat_mc, lowercase=TRUE)\n",
    "    processed_corpus = prepDocuments(processed_corpus_temp$documents,\n",
    "                                 processed_corpus_temp$vocab, \n",
    "                                 processed_corpus_temp$meta,\n",
    "                                 lower.thresh=1)\n",
    "    rm(processed_corpus_temp); invisible(gc())\n",
    "''');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "robjects.r('''\n",
    "    fit = stm(processed_corpus$documents, \n",
    "             processed_corpus$vocab,\n",
    "             K=20,\n",
    "             data=processed_corpus$meta,\n",
    "             init.type = 'Spectral',\n",
    "             seed=02138)\n",
    "''');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning Initialization.\n",
      "\t Calculating the gram matrix...\n",
      "\t Finding anchor words...\n",
      " \t....................\n",
      "\t Recovering initialization...\n",
      " \t....................................................................\n",
      "Initialization complete.\n"
     ]
    }
   ],
   "source": [
    "robjects.r('''\n",
    "    documents <- fit$documents\n",
    "    vocab <- fit$vocab\n",
    "    settings <- fit$settings \n",
    "    model <- fit$model\n",
    "    verbose <- settings$verbose\n",
    "  ##########\n",
    "  #Step 1: Initialize Parameters\n",
    "  ##########\n",
    "    ngroups <- settings$ngroups\n",
    "  if(is.null(model)) {\n",
    "    if(verbose) cat(\"Beginning Initialization.\\n\")\n",
    "    #initialize\n",
    "    model <- stm.init(documents, settings)\n",
    "    #if we were using the Lee and Mimno method of setting K, update the settings\n",
    "    if(settings$dim$K==0) settings$dim$K <- nrow(model$beta[[1]])\n",
    "    #unpack\n",
    "    mu <- list(mu=model$mu)\n",
    "    sigma <- model$sigma\n",
    "    beta <- list(beta=model$beta)\n",
    "    if(!is.null(model$kappa)) beta$kappa <- model$kappa\n",
    "    lambda <- model$lambda\n",
    "    convergence <- NULL \n",
    "    #discard the old object\n",
    "    rm(model)\n",
    "  } else {\n",
    "    if(verbose) cat(\"Restarting Model...\\n\")\n",
    "    #extract from a standard STM object so we can simply continue.\n",
    "    mu <- model$mu\n",
    "    beta <- list(beta=lapply(model$beta$logbeta, exp))\n",
    "    if(!is.null(model$beta$kappa)) beta$kappa <- model$beta$kappa\n",
    "    sigma <- model$sigma\n",
    "    lambda <- model$eta\n",
    "    convergence <- model$convergence\n",
    "    #manually declare the model not converged or it will stop after the first iteration\n",
    "    convergence$stopits <- FALSE\n",
    "    convergence$converged <- FALSE\n",
    "    #iterate by 1 as that would have happened otherwise\n",
    "    convergence$its <- convergence$its + 1 \n",
    "  }    \n",
    "  \n",
    "  #Pull out some book keeping elements\n",
    "  ntokens <- sum(settings$dim$wcounts$x)\n",
    "  betaindex <- settings$covariates$betaindex\n",
    "  stopits <- FALSE\n",
    "  if(ngroups!=1) {\n",
    "    groups <- cut(1:length(documents), breaks=ngroups, labels=FALSE) \n",
    "  }\n",
    "  suffstats <- vector(mode=\"list\", length=ngroups)\n",
    "''');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rlist_2py(rlist):\n",
    "    return dict(zip(rlist.names,\n",
    "               list(rlist)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fit = dict(zip( robjects.globalenv['fit'].names, \n",
    "         list( robjects.globalenv['fit'])))\n",
    "settings = dict(zip( fit['settings'].names, \n",
    "         list(fit['settings'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "K, A, V, N = [int(settings['dim'][i][0]) for i in range(4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Some setup for EM, retrieving the R objects\n",
    "stopits = False\n",
    "ngroups = int(robjects.globalenv['ngroups'][0])\n",
    "documents = [np.array(x) for x in list(robjects.globalenv['documents'])]\n",
    "beta_index = np.array(robjects.globalenv['betaindex'])\n",
    "beta = [np.array(x) for x in robjects.globalenv['beta'][0]]\n",
    "update_mu = True\n",
    "Lambda = np.array(robjects.globalenv['lambda'])\n",
    "mu = np.array(robjects.globalenv['mu'][0])\n",
    "sigma = np.array(robjects.globalenv['sigma'])\n",
    "verbose = settings['verbose'][0]\n",
    "\n",
    "# Run EM\n",
    "for i in range(2):\n",
    "    \n",
    "    # Non-blocked update\n",
    "    if ngroups==1:\n",
    "        # TODO\n",
    "        a = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Run all the necessary imports\n",
    "def run_imports(x):\n",
    "    import scipy as sp\n",
    "    import numpy as np\n",
    "    from scipy import optimize\n",
    "    return 1\n",
    "\n",
    "outs = sc.parallelize(range(100)).map(run_imports).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def likelihood(eta, beta, doc_ct, mu, siginv):\n",
    "    exp_eta = np.exp(np.append(eta, np.array([0])))\n",
    "    ndoc = np.sum(doc_ct)\n",
    "    part1 = np.dot(np.log(np.dot(exp_eta, beta)), doc_ct) - ndoc * np.log(np.sum(exp_eta))\n",
    "    diff = mu.T - eta\n",
    "    part2 = 0.5 * float(np.dot(np.dot(diff, siginv), diff.T))\n",
    "    return part2 - part1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def grad(eta, beta, doc_ct, mu, siginv):\n",
    "    exp_eta = np.exp(np.append(eta, [0]))\n",
    "    beta_prime = np.apply_along_axis(lambda x: x * exp_eta, 0, beta)\n",
    "    part1 = np.dot(beta_prime, doc_ct/np.sum(beta_prime, 0).T) - (np.sum(doc_ct)/ np.sum(exp_eta)) * exp_eta\n",
    "    diff = mu.T - eta\n",
    "    part2 = np.dot(siginv, diff.T)\n",
    "    part1 = part1[:len(part1)-1]\n",
    "    return (part2.T - part1).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def estep_docloop(doc_item, siginv, sigmaentropy):\n",
    "    doc_ct = doc_item['doc'][1]\n",
    "    eta = doc_item['init']\n",
    "    beta = doc_item['beta_i']\n",
    "    mu = doc_item['mu_i']\n",
    "    optim_par = sp.optimize.minimize(likelihood, eta, args=(beta, doc_ct, mu, siginv), \n",
    "                            method='BFGS')\n",
    "    \n",
    "    def hpb(eta, beta, doc_ct, mu, siginv, sigmaentropy):\n",
    "        \n",
    "        # Compute the Hessian\n",
    "        exp_eta = np.exp(np.append(eta, [0]))\n",
    "        theta = np.reshape(exp_eta/np.sum(exp_eta), (len(exp_eta), -1)).T\n",
    "        EB = np.apply_along_axis(lambda x: x * exp_eta, 0, beta)\n",
    "        EB = np.apply_along_axis(lambda x: x * (np.sqrt(doc_ct).T) / np.sum(EB,0), 1, EB)\n",
    "        hess = np.dot(EB, EB.T) - np.sum(doc_ct) * np.dot(theta.T, theta)    \n",
    "        EB = np.apply_along_axis(lambda x: x * np.sqrt(doc_ct).T, 1, EB)\n",
    "        hess[np.diag_indices_from(hess)] = hess[np.diag_indices_from(hess)] - np.sum(EB, 1) + np.sum(doc_ct) * theta\n",
    "        hess = hess[:hess.shape[0]-1,:hess.shape[1]-1] + siginv\n",
    "\n",
    "        # Invert via Cholesky decomposition\n",
    "        try:\n",
    "            nu = np.linalg.cholesky(hess)\n",
    "        except:\n",
    "            dvec = np.array(np.diag(hess))\n",
    "            magnitudes = np.sum(np.abs(hess), 1) - abs(dvec)\n",
    "            Km1 = len(dvec)\n",
    "            for i in range(Km1):\n",
    "                if dvec[i] < magnitudes[i]:\n",
    "                    dvec[i] = magnitudes[i]\n",
    "            hess[np.diag_indices_from(hess)] = dvec\n",
    "            nu = np.linalg.cholesky(hess)\n",
    "\n",
    "        # Finish construction\n",
    "        det_term = -np.sum(np.log(np.diag(nu)))\n",
    "        nu = np.linalg.inv(np.triu(nu))\n",
    "        nu = np.dot(nu, nu.T)\n",
    "        diff = eta - mu.flatten()\n",
    "\n",
    "        # Compute the bound\n",
    "        bound = (np.dot(np.log(np.dot(theta, beta)), doc_ct) + det_term \n",
    "                 - 0.5 * np.dot(diff.T, np.dot(siginv, diff)) - sigmaentropy)\n",
    "\n",
    "        # Construct ouput\n",
    "        out = {'phis': EB,\n",
    "               'eta': {'lambda': eta, 'nu': nu},\n",
    "               'bound': bound}\n",
    "        return out\n",
    "    \n",
    "    return hpb(optim_par.x, beta, doc_ct, mu, siginv, sigmaentropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def estep_spark(documents, beta_index, beta, Lambda_old,\n",
    "                mu, sigma, verbose, sc, update_mu=False):\n",
    "    \n",
    "    # Initialize sufficient statistics\n",
    "    sigma_ss = np.zeros((K-1, K-1))\n",
    "    beta_ss = [np.zeros((K, V)) for i in range(A)]\n",
    "    bound = np.array([0] * N)\n",
    "    Lambda = np.array([0] * N)\n",
    "    siginv = np.linalg.inv(sigma)\n",
    "    sigmaentropy = np.log(np.abs(np.linalg.det(sigma))) * 0.5\n",
    "    \n",
    "    # Parallelize document collection\n",
    "    collection = [{'doc':doc, 'aspect': int(aspect), 'init': init} \n",
    "                  for (doc, aspect, init) in zip(documents, beta_index, Lambda_old)]\n",
    "    for item in collection:\n",
    "        item['beta_i'] = beta[item['aspect']-1][:,[x-1 for x in item['doc'][0]]]\n",
    "        item['mu_i'] = mu\n",
    "    collection_par = sc.parallelize(collection)\n",
    "    \n",
    "    return collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# These operations wil eventually be run within estep_spark; just here for speed's sake\n",
    "collection = estep_spark(documents, beta_index, beta, Lambda, mu, sigma, verbose, sc, False)\n",
    "collection_par = sc.parallelize(collection[:1])\n",
    "collection_par.collect()\n",
    "trial_run = collection_par.map(lambda x: estep_docloop(x, siginv, sigmaentropy)).collect()\n",
    "print trial_run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STM Class Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas.rpy.common as rcom\n",
    "iris = rcom.load_data('iris')\n",
    "iris.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg.distributed import CoordinateMatrix, MatrixEntry\n",
    "\n",
    "class STM(object):\n",
    "    \n",
    "    def __init__(self, sc):\n",
    "        self.sc = sc\n",
    "        self.status = 0\n",
    "        self.theta = None\n",
    "        self.n_partitions = None # Get this from Spark context\n",
    "        self.lhood_bound = None\n",
    "        self.seed = None\n",
    "        self.description = None\n",
    "    \n",
    "    def __e_step__(self, documents):\n",
    "        mu_i = self.mu\n",
    "        sigma_ss = np.zeros((self.K-1, self.K-1))\n",
    "        beta_ss = np.zeros((self.A, self.K, self.V))\n",
    "        bound = np.array([0] * N)\n",
    "        Lambda = np.array([0] * N)\n",
    "        siginv = np.linalg.inv(self.sigma)\n",
    "        \n",
    "        \n",
    "    def __m_step__(self):\n",
    "        self.stopits = True\n",
    "        \n",
    "    def __optimize_stm__(self, optimizer):\n",
    "        if optimizer == 'em':\n",
    "            # Split documents into groups\n",
    "            self.groups = [[x for x in range(self.N) if x % self.ngroups == j] \n",
    "                           for j in range(self.ngroups)]\n",
    "            \n",
    "            # Run EM\n",
    "            print \"Beginning EM\"\n",
    "            while self.stopits == False:\n",
    "                #for i in xrange(self.ngroups):\n",
    "                #    gdocs = self.documents[self.groups[i]]\n",
    "                #    self.__e_step__(gdocs)\n",
    "                #    if self.verbose:\n",
    "                #        print \"Completed Group \" + str(i) + \" E-Step\"\n",
    "                self.__e_step__(documents)\n",
    "                self.__m_step__()\n",
    "    \n",
    "    def __theta_posterior_draw__(self):\n",
    "        # TODO\n",
    "        return None\n",
    "    \n",
    "    def __compute_lhood_bound__(self):\n",
    "        # TODO\n",
    "        return None\n",
    "    \n",
    "    def __init_random__(self):\n",
    "        self.mu = np.zeros((self.K-1, 1))\n",
    "        self.sigma = 20 * np.identity(self.K-1)\n",
    "        self.beta = np.random.gamma(shape=0.1, size=(self.A, self.K, self.V))\n",
    "        self.Lambda = np.zeros((self.N, self.K-1))\n",
    "        \n",
    "    def __kappa_init__(self):\n",
    "        \n",
    "        # Baseline log probabilities\n",
    "        m = np.array(self.wcounts)[0].astype(float)/ np.sum(np.array(self.wcounts)[0])\n",
    "        m = np.log(m) - np.log(m)\n",
    "        self.kappa['m'] = m; del m;\n",
    "        \n",
    "        # Parameter objects\n",
    "        self.aspectmod = self.A > 1\n",
    "        \n",
    "        # Covariates\n",
    "        self.kappa['covar'] = None\n",
    "        \n",
    "        \n",
    "    def print_topics(self):\n",
    "        print self.groups\n",
    "    \n",
    "    def estimate_effect(self):\n",
    "        # TODO\n",
    "        self.N = 30\n",
    "        return None\n",
    "    \n",
    "    def train(self, documents, vocab=None, K=10, prevalence=None, content=None, data=None,\n",
    "              max_em_its=500, init_type=\"random\", optimizer=\"em\", \n",
    "              em_tol=1e-5, verbose=True, report_every=5, LDAbeta=True,\n",
    "              interactions=True, ngroups=10, model=None):\n",
    "        \"\"\"Train a STM model.\n",
    "        \"\"\"\n",
    "        # Input parsing\n",
    "        if type(documents) == sp.sparse.coo_matrix:\n",
    "            self.documents = documents.tolil()\n",
    "            #self.documents = documents\n",
    "            \n",
    "        # Parallelize the collection\n",
    "        #self.dist_docs = CoordinateMatrix(sc.parallelize([MatrixEntry(i, j, x) for i, j, x \n",
    "        #                    in zip(documents.row, documents.col, documents.data)]))\n",
    "        #print self.dist_docs.numCols(), self.dist_docs.numRows()\n",
    "        #self.dist_\n",
    "        \n",
    "        # Basic setup\n",
    "        self.N, self.V = documents.shape\n",
    "        self.K = K\n",
    "        self.verbose = verbose\n",
    "        self.report_every = report_every\n",
    "        self.ngroups = ngroups\n",
    "        self.max_em_its = max_em_its\n",
    "        self.em_tol = em_tol\n",
    "        self.gamma = {\n",
    "            'mode' : 'L1',\n",
    "            'prior' : None,\n",
    "            'enet' : 1\n",
    "        }\n",
    "        self.kappa = {\n",
    "            'LDAbeta' : LDAbeta,\n",
    "            'interactions' : interactions,\n",
    "            'fixedintercept' : True,\n",
    "            'mstep' : {\n",
    "                'tol' : .001,\n",
    "                'maxit' : 3\n",
    "            },\n",
    "            'contrasts' : False\n",
    "        }\n",
    "        self.tau = {\n",
    "            'mode' : 'L1',\n",
    "            'nits' : 50,\n",
    "            'burnin' : 25,\n",
    "            'alpha' : 50.0 / self.K,\n",
    "            'eta' : .01,\n",
    "            's' : .05,\n",
    "            'p' : 3000,\n",
    "            'd_groups_size' : 2000\n",
    "        }\n",
    "        self.wcounts = documents.sum(0)\n",
    "        self.ntokens = np.sum(self.wcounts)\n",
    "        self.stopits = False\n",
    "        self.A = len(content)\n",
    "        self.P = len(prevalence)\n",
    "        self.X = data[prevalence]\n",
    "        self.Y = data[content]\n",
    "                \n",
    "        # Initialization\n",
    "        print \"Beginning Initialization\"\n",
    "        if init_type == 'random':\n",
    "            self.__init_random__()\n",
    "        self.__kappa_init__()\n",
    "        \n",
    "        # Run EM\n",
    "        self.__optimize_stm__(optimizer)\n",
    "        \n",
    "        # Declare completion\n",
    "        print \"All done!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stm = STM(sc)\n",
    "stm.train(documents, prevalence=['Species'], content=['Species'], data=iris, ngroups=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "documents_lil = documents.tolil()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sc.parallelize(documents_lil.todense()).map(hi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "[(i, j, x) for i, j, x in zip(documents.row, documents.col, documents.data)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "myvec = sc.parallelize([[1,2,3], [4,5,6,7], [8,9]])\n",
    "myvec.map(do_something).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def do_something(x):\n",
    "    return str(sp.stats.hmean(x)) + \" hello\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Scraps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Step 1: Initalize Parameters\n",
    "#ngroups = int(settings['ngroups'][0])\n",
    "#if fit['model'] == rpy2.rinterface.NULL:\n",
    "#    print \"Beginning Initialization\"\n",
    "#    model = robjects.globalenv['stm.init'](fit['documents'], fit['settings'])\n",
    "#    if K == 0:\n",
    "        \n",
    "#else:\n",
    "#    print \"Restarting Model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#settings = dict(zip( fit['settings'].names, \n",
    "#         list(fit['settings'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "# Save the objects for debugging\n",
    "#save(documents, betaindex, beta, lambda, mu, sigma, verbose, file=\"estep_prep.RData\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "exp_eta = np.exp(doc_item['init'])\n",
    "eta = doc_item['init']\n",
    "betas = doc_item['beta_i']\n",
    "doc_ct = doc_item['doc'][1]\n",
    "ndoc = np.sum(doc_ct)\n",
    "siginv = np.linalg.inv(sigma)\n",
    "part1 = np.dot(np.log(np.dot(exp_eta, betas)), doc_ct) - ndoc * np.log(np.sum(exp_eta))\n",
    "diff = mu.T - eta[:K-1]\n",
    "part2 = 0.5 * float(np.dot(np.dot(diff, siginv), diff.T))\n",
    "#ndoc = np.sum(doc_ct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doc_item = collection[1]\n",
    "doc_ct = doc_item['doc'][1]\n",
    "ndoc = np.sum(doc_ct)\n",
    "ndoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sigmaentropy = np.log(np.abs(np.linalg.det(sigma))) * 0.5\n",
    "hi = hpb(eta=optim_par, beta=doc_item['beta_i'], doc_ct=doc_item['doc'][1],\n",
    "           mu=mu, siginv=siginv, sigmaentropy=sigmaentropy)\n",
    "\n",
    "siginv = np.linalg.inv(sigma)\n",
    "likelihood(eta=doc_item['init'], beta=doc_item['beta_i'], doc_ct=doc_item['doc'][1],\n",
    "           mu=mu, siginv=siginv)\n",
    "\n",
    "grad(eta=doc_item['init'], beta=doc_item['beta_i'], doc_ct=doc_item['doc'][1],\n",
    "           mu=mu, siginv=siginv)\n",
    "\n",
    "eps = [1e-8] * 19\n",
    "numerical_gradient = optimize.approx_fprime(doc_item['init'], likelihood, \n",
    "                                            eps, doc_item['beta_i'], doc_item['doc'][1], mu, siginv)\n",
    "numerical_gradient\n",
    "\n",
    "# Try optimization\n",
    "eta=doc_item['init']\n",
    "beta=doc_item['beta_i']\n",
    "doc_ct=doc_item['doc'][1]\n",
    "mu=mu\n",
    "siginv = np.linalg.inv(sigma)\n",
    "\n",
    "test = sp.optimize.minimize(likelihood, eta, args=(beta, doc_ct, mu, siginv), \n",
    "                            method='BFGS')\n",
    "optim_par = test.x\n",
    "\n",
    "doc_item.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Input the data\n",
    "data = sc.textFile(\"sample_data.txt\")\n",
    "parsedData = data.map(lambda line: Vectors.dense([float(x) for x in line.strip().split(' ')]))\n",
    "corpus = parsedData.zipWithIndex().map(lambda x: [x[1], x[0]]).cache()\n",
    "\n",
    "# Cluster the documents into three topics using LDA\n",
    "ldaModel = LDA.train(corpus, k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Output topics. Each is a distribution over words (matching word count vectors)\n",
    "print(\"Learned topics (as distributions over vocab of \" + str(ldaModel.vocabSize()) + \" words):\")\n",
    "topics = ldaModel.topicsMatrix()\n",
    "for topic in range(3):\n",
    "    print(\"Topic \" + str(topic) + \":\")\n",
    "    for word in range(0, ldaModel.vocabSize()):\n",
    "        print(\" \" + str(topics[word][topic]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Sample docs\n",
    "documents = sp.io.mmread(\"sample_docs_sparse.obj\")\n",
    "N, V = documents.shape\n",
    "print N, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Need to figure out prevalence\n",
    "robjects.r('''\n",
    "    fit = stm(processed_corpus$documents, \n",
    "             processed_corpus$vocab, \n",
    "             K=20, prevalence=~s(date), \n",
    "             data=processed_corpus$meta,\n",
    "             init.type = 'Spectral',\n",
    "             seed=02138)\n",
    "''');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def hpb(eta, beta, doc_ct, mu, siginv, sigmaentropy):\n",
    "        \n",
    "    # Compute the Hessian\n",
    "    exp_eta = np.exp(np.append(eta, [0]))\n",
    "    theta = np.reshape(exp_eta/np.sum(exp_eta), (len(exp_eta), -1)).T\n",
    "    EB = np.apply_along_axis(lambda x: x * exp_eta, 0, beta)\n",
    "    EB = np.apply_along_axis(lambda x: x * (np.sqrt(doc_ct).T) / np.sum(EB,0), 1, EB)\n",
    "    hess = np.dot(EB, EB.T) - np.sum(doc_ct) * np.dot(theta.T, theta)    \n",
    "    EB = np.apply_along_axis(lambda x: x * np.sqrt(doc_ct).T, 1, EB)\n",
    "    hess[np.diag_indices_from(hess)] = hess[np.diag_indices_from(hess)] - np.sum(EB, 1) + np.sum(doc_ct) * theta\n",
    "    hess = hess[:hess.shape[0]-1,:hess.shape[1]-1] + siginv\n",
    "    \n",
    "    # Invert via Cholesky decomposition\n",
    "    try:\n",
    "        nu = np.linalg.cholesky(hess)\n",
    "    except:\n",
    "        dvec = np.array(np.diag(hess))\n",
    "        magnitudes = np.sum(np.abs(hess), 1) - abs(dvec)\n",
    "        Km1 = len(dvec)\n",
    "        for i in range(Km1):\n",
    "            if dvec[i] < magnitudes[i]:\n",
    "                dvec[i] = magnitudes[i]\n",
    "        hess[np.diag_indices_from(hess)] = dvec\n",
    "        nu = np.linalg.cholesky(hess)\n",
    "    \n",
    "    # Finish construction\n",
    "    det_term = -np.sum(np.log(np.diag(nu)))\n",
    "    nu = np.linalg.inv(np.triu(nu))\n",
    "    nu = np.dot(nu, nu.T)\n",
    "    diff = eta - mu.flatten()\n",
    "    \n",
    "    # Compute the bound\n",
    "    bound = (np.dot(np.log(np.dot(theta, beta)), doc_ct) + det_term \n",
    "             - 0.5 * np.dot(diff.T, np.dot(siginv, diff)) - sigmaentropy)\n",
    "    \n",
    "    # Construct ouput\n",
    "    out = {'phis': EB,\n",
    "           'eta': {'lambda': eta, 'nu': nu},\n",
    "           'bound': bound}\n",
    "    return out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pySpark (Spark 1.5.0)",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
